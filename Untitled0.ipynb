{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justforyou2017/Deep-Learning-with-PyTorch-Tutorials/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcILeIthC8Jq",
        "colab_type": "code",
        "outputId": "b627daf8-230f-4a78-c4a1-fc80932aceee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from sklearn.preprocessing.data import MinMaxScaler\n",
        "import matplotlib as mpl\n",
        "def dt_to_timestamp(dt):\n",
        "    # timestamp = dt.timestamp()  # 对于 python 3 可以直接使用 timestamp 获取时间戳\n",
        "    timestamp = (dt - datetime.fromtimestamp(0)).total_seconds()  # Python 2 需手动换算\n",
        "    return timestamp\n",
        "\n",
        "def get_hourly_chime(dt, step=0, rounding_level=\"s\"):\n",
        "    \"\"\"\n",
        "    计算整分钟，整小时，整天的时间\n",
        "    :param step: 往前或往后跳跃取整值，默认为0，即当前所在的时间，正数为往后，负数往前。\n",
        "                例如：\n",
        "                step = 0 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:21\n",
        "                step = 1 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:22\n",
        "                step = -1 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:20\n",
        "    :param rounding_level: 字符串格式。\n",
        "                \"s\": 按秒取整；\"min\": 按分钟取整；\"hour\": 按小时取整；\"days\": 按天取整\n",
        "    :return: 整理后的时间戳\n",
        "    \"\"\"\n",
        "    if rounding_level == \"days\":  # 整天\n",
        "        td = timedelta(days=-step, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=dt.minute, hours=dt.hour, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"hour\":  # 整小时\n",
        "        td = timedelta(days=0, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=dt.minute, hours=-step, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"min\":  # 整分钟\n",
        "        td = timedelta(days=0, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=-step, hours=0, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"s\":  # 整秒\n",
        "        td = timedelta(days=0, seconds=-step, microseconds=dt.microsecond, milliseconds=0, minutes=0, hours=0, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    else:\n",
        "        new_dt = dt\n",
        "    # timestamp = new_dt.timestamp()  # 对于 python 3 可以直接使用 timestamp 获取时间戳\n",
        "    timestamp = (new_dt - datetime.fromtimestamp(0)).total_seconds()  # Python 2 需手动换算\n",
        "    return timestamp\n",
        "def myappl(data):\n",
        "    if data['时间'] !=None:\n",
        "        t =time.mktime(time.strptime(data['时间'], \"%Y/%m/%d %H:%M\"))\n",
        "        second = get_hourly_chime(dt=datetime.fromtimestamp(t), step=0, rounding_level=\"hour\")\n",
        "    return second\n",
        "def myappl2(data):\n",
        "    if 'CO' in data.index :\n",
        "        return time.strftime(\"%Y/%m/%d %H:%M\", time.localtime(data.name))\n",
        "\n",
        "def load_data(data, seq_len):\n",
        "    print('data len:',len(data))       #4172\n",
        "    print('sequence len:',seq_len)     #50\n",
        " \n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])  #得到长度为seq_len+1的向量，最后一个作为label\n",
        " \n",
        "    print('result len:',len(result))   #4121\n",
        "    print('result shape:',np.array(result).shape)  #（4121,51）\n",
        " \n",
        "    result = np.array(result,dtype= np.float32)\n",
        "    #划分train、test\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    \n",
        "    \n",
        "    train = torch.from_numpy(result[:row])\n",
        "    test = torch.from_numpy(result[row:])\n",
        "    return train, test\n",
        "\n",
        "def pic(y_test,y_hat,name):\n",
        "    mpl.rcParams['font.sans-serif'] = ['simHei']\n",
        "    mpl.rcParams['axes.unicode_minus'] = False\n",
        "    plt.figure(figsize=(10,8),facecolor='w')\n",
        "    plt.plot(y_test, 'r-', linewidth=2, label='origional')\n",
        "    plt.plot(y_hat, 'g-', linewidth=2, label='prediction')\n",
        "    plt.title(name, fontsize=18)\n",
        "#     plt.xticks([])\n",
        "#     plt.yticks([])\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(b=True, ls=':')\n",
        "     \n",
        "def train_prediction(train ,test,learning_rate,name):\n",
        "    hidden_size =16 \n",
        "    seq_length = 1 \n",
        "    batch_size = 51\n",
        "    last =  train[0][-1],train[1][-1]\n",
        "    all = train[0][:,-1],train[1][:,-1]\n",
        "       \n",
        "    model = Lstm(train[0].shape[2] ,hidden_size)\n",
        "    print(model)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "     #out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "#     hidden_prev = torch.zeros(4, batch_size, hidden_size)\n",
        "    for i in range(len(train[0])):\n",
        "        x = train[0][i].view(batch_size, 1 , -1)\n",
        "        y = train[1][i].view(batch_size, seq_length , 1)\n",
        "        output, hidden_prev = model(x)\n",
        "#         hidden_prev = hidden_prev.detach()\n",
        "        \n",
        "        loss = criterion(output, y) # 用前49个点去学习，并用学得的曲线输出即为后49个点并与后49个点真值进行求损失\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        # for p in model.parameters():\n",
        "        #     print(p.grad.norm())\n",
        "        # torch.nn.utils.clip_grad_norm_(p, 10)\n",
        "        optimizer.step()\n",
        "        if i % 200 == 0:\n",
        "            print(\"Iteration: {} \\t loss {}\".format(i, loss.item()))\n",
        "    predictions = []\n",
        "    orig = []\n",
        "    \n",
        "    \n",
        "    \n",
        "    pred, _ = model(all[0].unsqueeze(1)) \n",
        "    predictions.extend(pred.squeeze().detach().numpy())\n",
        "    orig.extend(all[1].numpy())\n",
        "    minput =all[0][-1]\n",
        "    for i in range(24):\n",
        "        minput = minput.view(1, 1, -1)\n",
        "        pred,_ = model(minput)\n",
        "        minput = minput+np.random.rand()\n",
        "        predictions.append(pred.detach().numpy().ravel()[0]) \n",
        "    dd = pd.DataFrame(data=list(zip(orig,predictions)) ,columns=[ '真实','预测'])\n",
        "     \n",
        "    dd.to_csv('{}_{}.csv'.format(name,np.random.randint(1,6,1)))\n",
        "    pic(orig,predictions,name)\n",
        "\n",
        "class Lstm(nn.Module):\n",
        "    def __init__(self,INPUT_SIZE,hidden_size):\n",
        "        super(Lstm,self).__init__()\n",
        "        self.lstm = nn.LSTM( \n",
        "            input_size=INPUT_SIZE,\n",
        "            hidden_size=hidden_size,     # rnn hidden unit\n",
        "            num_layers=4,               # number of rnn layer\n",
        "            batch_first=True,           # input & output will has (batch,1s dimension. e.g.\n",
        "            dropout=0.2                 #(batch, time_step, input_size) time_step, input_size)\n",
        "        )\n",
        "        for p in self.lstm.parameters():\n",
        "            nn.init.normal_(p, mean=0.0, std=0.001)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x):    \n",
        "        out, (hidden_prev, c) = self.lstm(x)  #hidden_prev.shape[layer, batch_size, hidden_size]\n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        t = out.size(0)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        out = out.unsqueeze(-1)\n",
        "        return out, (hidden_prev, c)\n",
        "  \n",
        "\n",
        "# Hyper Parameters\n",
        "torch.manual_seed(1)    # reproducible\n",
        "TIME_STEP = 1      # rnn time step\n",
        "INPUT_SIZE = 1      # rnn input size\n",
        "hidden_size = 16\n",
        "LR = 0.02           # learning rate\n",
        "\n",
        " \n",
        "data1 = pd.read_csv('/content/drive/My Drive/tmp/附件1.csv')\n",
        "data2 = pd.read_csv('/content/drive/My Drive/tmp/附件2.csv') \n",
        "data2['hsecond'] = data2.apply(myappl, axis=1)\n",
        "\n",
        "output_size =1\n",
        "learning_rate = 0.01\n",
        "num_epochs =1\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train1,d_test1 = load_data(result, 50 )\n",
        "x_train1, y_train1, x_test1, y_test1  = d_train1[:,:,1:],d_train1[:,:,0],d_test1[:,:,1:],d_test1[:,:,0]\n",
        " \n",
        "train_prediction((x_train1, y_train1) ,(x_test1, y_test1),learning_rate,'PM2.5')\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['PM10']\n",
        "ddd.drop(['PM10'],axis=1,inplace=True)\n",
        "ddd['PM10'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'PM10')\n",
        "\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['CO']\n",
        "ddd.drop(['CO'],axis=1,inplace=True)\n",
        "ddd['CO'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'CO')\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['NO2']\n",
        "ddd.drop(['NO2'],axis=1,inplace=True)\n",
        "ddd['NO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'NO2')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['SO2']\n",
        "ddd.drop(['SO2'],axis=1,inplace=True)\n",
        "ddd['SO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'SO2')\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['O3']\n",
        "ddd.drop(['O3'],axis=1,inplace=True)\n",
        "ddd['O3'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'O3')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data2 = data2.groupby('hsecond').mean()\n",
        "# data2['时间']  = data2.apply(myappl2,axis=1)\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train2 , d_test2 = load_data(result, 50)\n",
        "x_train2, y_train2, x_test2, y_test2  = d_train2[:,:,1:11],d_train2[:,:,0],d_test2[:,:,1:11],d_test2[:,:,0]\n",
        "\n",
        "train_prediction((x_train2, y_train2) ,(x_test2, y_test2 ),learning_rate,'PM2.5')\n",
        "\n",
        "\n",
        "tmp = data2['PM10']\n",
        "data2.drop(['PM10'],axis=1,inplace=True)\n",
        "data2['PM10'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'PM10')   \n",
        "        \n",
        "\n",
        "tmp = data2['CO']\n",
        "data2.drop(['CO'],axis=1,inplace=True)\n",
        "data2['CO'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'CO')     \n",
        "  \n",
        "\n",
        "\n",
        "tmp = data2['NO2']\n",
        "data2.drop(['NO2'],axis=1,inplace=True)\n",
        "data2['NO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'NO2')   \n",
        "        \n",
        "\n",
        "tmp = data2['SO2']\n",
        "data2.drop(['SO2'],axis=1,inplace=True)\n",
        "data2['SO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'SO2')      \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "tmp = data2['O3']\n",
        "data2.drop(['O3'],axis=1,inplace=True)\n",
        "data2['O3'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'O3')   \n",
        "        \n",
        "\n",
        "tmp = data2['风速']\n",
        "data2.drop(['风速'],axis=1,inplace=True)\n",
        "data2['风速'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'windSpeed')    \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "tmp = data2['压强']\n",
        "data2.drop(['压强'],axis=1,inplace=True)\n",
        "data2['压强'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'pressure')   \n",
        "        \n",
        "\n",
        "tmp = data2['降水量']\n",
        "data2.drop(['降水量'],axis=1,inplace=True)\n",
        "data2['降水量'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'precipitation')    \n",
        "  \n",
        "  \n",
        "tmp = data2['温度']\n",
        "data2.drop(['温度'],axis=1,inplace=True)\n",
        "data2['温度'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'temperature')   \n",
        "        \n",
        "\n",
        "tmp = data2['湿度']\n",
        "data2.drop(['湿度'],axis=1,inplace=True)\n",
        "data2['湿度'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'humidity') \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#PM2.5,PM10,CO,NO2,SO2,O3,风速,压强,降水量,温度,湿度\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "# x = torch.randn(10, 3, 100)\n",
        "# out, (h, c) = lstm(x)\n",
        "# print(out.shape, h.shape, c.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.004163509234786034\n",
            "Iteration: 200 \t loss 0.0022360114380717278\n",
            "Iteration: 400 \t loss 0.004413819871842861\n",
            "Iteration: 600 \t loss 0.002702730242162943\n",
            "Iteration: 800 \t loss 0.0011058201780542731\n",
            "Iteration: 1000 \t loss 0.0003594347508624196\n",
            "Iteration: 1200 \t loss 0.0011999821290373802\n",
            "Iteration: 1400 \t loss 0.003111378289759159\n",
            "Iteration: 1600 \t loss 0.0016811952227726579\n",
            "Iteration: 1800 \t loss 0.0026465619448572397\n",
            "Iteration: 2000 \t loss 0.0003380585403647274\n",
            "Iteration: 2200 \t loss 0.002430796390399337\n",
            "Iteration: 2400 \t loss 0.0030681700445711613\n",
            "Iteration: 2600 \t loss 0.0012634714366868138\n",
            "Iteration: 2800 \t loss 0.0017792786238715053\n",
            "Iteration: 3000 \t loss 0.0007517727208323777\n",
            "Iteration: 3200 \t loss 0.0018285379046574235\n",
            "Iteration: 3400 \t loss 0.0009693647152744234\n",
            "Iteration: 3600 \t loss 0.0006049996591173112\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.005552304908633232\n",
            "Iteration: 200 \t loss 0.00039171887328848243\n",
            "Iteration: 400 \t loss 0.000287643721094355\n",
            "Iteration: 600 \t loss 0.00013159493391867727\n",
            "Iteration: 800 \t loss 0.00019252835772931576\n",
            "Iteration: 1000 \t loss 0.00012545128993224353\n",
            "Iteration: 1200 \t loss 9.872904774965718e-05\n",
            "Iteration: 1400 \t loss 0.00019426480866968632\n",
            "Iteration: 1600 \t loss 0.00010315011604689062\n",
            "Iteration: 1800 \t loss 0.0002926344168372452\n",
            "Iteration: 2000 \t loss 6.601947825402021e-05\n",
            "Iteration: 2200 \t loss 0.0003308110754005611\n",
            "Iteration: 2400 \t loss 0.00027792443870566785\n",
            "Iteration: 2600 \t loss 0.0001213279101648368\n",
            "Iteration: 2800 \t loss 0.0001777450233930722\n",
            "Iteration: 3000 \t loss 7.809011003701016e-05\n",
            "Iteration: 3200 \t loss 0.015385978855192661\n",
            "Iteration: 3400 \t loss 0.0003371565544512123\n",
            "Iteration: 3600 \t loss 0.000257766165304929\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.015298710204660892\n",
            "Iteration: 200 \t loss 0.0024048862978816032\n",
            "Iteration: 400 \t loss 0.004251732956618071\n",
            "Iteration: 600 \t loss 0.0023231019731611013\n",
            "Iteration: 800 \t loss 0.0022901520133018494\n",
            "Iteration: 1000 \t loss 0.0014786553801968694\n",
            "Iteration: 1200 \t loss 0.0020308280363678932\n",
            "Iteration: 1400 \t loss 0.0025947117246687412\n",
            "Iteration: 1600 \t loss 0.0011744166258722544\n",
            "Iteration: 1800 \t loss 0.002624065848067403\n",
            "Iteration: 2000 \t loss 0.0012193989241495728\n",
            "Iteration: 2200 \t loss 0.003020991338416934\n",
            "Iteration: 2400 \t loss 0.009090843610465527\n",
            "Iteration: 2600 \t loss 0.0043446701020002365\n",
            "Iteration: 2800 \t loss 0.0031025325879454613\n",
            "Iteration: 3000 \t loss 0.001257537747733295\n",
            "Iteration: 3200 \t loss 0.0016198176890611649\n",
            "Iteration: 3400 \t loss 0.001405927469022572\n",
            "Iteration: 3600 \t loss 0.0005104744923301041\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.004975159652531147\n",
            "Iteration: 200 \t loss 0.0005398556822910905\n",
            "Iteration: 400 \t loss 4.843859278480522e-05\n",
            "Iteration: 600 \t loss 0.0002393425820628181\n",
            "Iteration: 800 \t loss 0.00010346915223635733\n",
            "Iteration: 1000 \t loss 0.0004894891171716154\n",
            "Iteration: 1200 \t loss 0.004246922675520182\n",
            "Iteration: 1400 \t loss 0.008207590319216251\n",
            "Iteration: 1600 \t loss 0.01043765340000391\n",
            "Iteration: 1800 \t loss 0.005354444030672312\n",
            "Iteration: 2000 \t loss 0.002663718070834875\n",
            "Iteration: 2200 \t loss 0.009279574267566204\n",
            "Iteration: 2400 \t loss 0.012942426837980747\n",
            "Iteration: 2600 \t loss 0.017434369772672653\n",
            "Iteration: 2800 \t loss 0.016124192625284195\n",
            "Iteration: 3000 \t loss 0.007715069688856602\n",
            "Iteration: 3200 \t loss 0.00804079044610262\n",
            "Iteration: 3400 \t loss 0.00467778230085969\n",
            "Iteration: 3600 \t loss 0.007447561714798212\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.01498192548751831\n",
            "Iteration: 200 \t loss 0.014256632886826992\n",
            "Iteration: 400 \t loss 0.004748198203742504\n",
            "Iteration: 600 \t loss 0.007738957181572914\n",
            "Iteration: 800 \t loss 0.003029612824320793\n",
            "Iteration: 1000 \t loss 0.004602394066751003\n",
            "Iteration: 1200 \t loss 0.0012897650012746453\n",
            "Iteration: 1400 \t loss 0.00018040169379673898\n",
            "Iteration: 1600 \t loss 0.0002517569519113749\n",
            "Iteration: 1800 \t loss 0.0001489127753302455\n",
            "Iteration: 2000 \t loss 0.00017321744235232472\n",
            "Iteration: 2200 \t loss 0.0003746233705896884\n",
            "Iteration: 2400 \t loss 4.1143201087834314e-05\n",
            "Iteration: 2600 \t loss 0.0007650107727386057\n",
            "Iteration: 2800 \t loss 0.0009992563864216208\n",
            "Iteration: 3000 \t loss 0.00021039263810962439\n",
            "Iteration: 3200 \t loss 0.0002499820839148015\n",
            "Iteration: 3400 \t loss 0.0005392185994423926\n",
            "Iteration: 3600 \t loss 0.00033417780650779605\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.0069947801530361176\n",
            "Iteration: 200 \t loss 0.00580115569755435\n",
            "Iteration: 400 \t loss 0.000621150596998632\n",
            "Iteration: 600 \t loss 0.0029732452239841223\n",
            "Iteration: 800 \t loss 0.0014962280401960015\n",
            "Iteration: 1000 \t loss 0.0012164557119831443\n",
            "Iteration: 1200 \t loss 0.0024590077809989452\n",
            "Iteration: 1400 \t loss 0.005077512003481388\n",
            "Iteration: 1600 \t loss 0.0019983076490461826\n",
            "Iteration: 1800 \t loss 0.006609502248466015\n",
            "Iteration: 2000 \t loss 0.0008043267298489809\n",
            "Iteration: 2200 \t loss 0.007269037887454033\n",
            "Iteration: 2400 \t loss 0.002561806235462427\n",
            "Iteration: 2600 \t loss 0.004090214613825083\n",
            "Iteration: 2800 \t loss 0.006038290448486805\n",
            "Iteration: 3000 \t loss 0.006971579510718584\n",
            "Iteration: 3200 \t loss 0.009642375633120537\n",
            "Iteration: 3400 \t loss 0.011844287626445293\n",
            "Iteration: 3600 \t loss 0.015446143224835396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHzP06oVDNJb",
        "colab_type": "code",
        "outputId": "62161622-54fc-4b2a-e48b-77a940f00e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}