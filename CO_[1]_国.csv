{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justforyou2017/Deep-Learning-with-PyTorch-Tutorials/blob/master/CO_%5B1%5D_%E5%9B%BD.csv\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcILeIthC8Jq",
        "colab_type": "code",
        "outputId": "d2843aba-ff6a-40f9-c4f4-92114803f5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from sklearn.preprocessing.data import MinMaxScaler\n",
        "import matplotlib as mpl\n",
        "def dt_to_timestamp(dt):\n",
        "    # timestamp = dt.timestamp()  # 对于 python 3 可以直接使用 timestamp 获取时间戳\n",
        "    timestamp = (dt - datetime.fromtimestamp(0)).total_seconds()  # Python 2 需手动换算\n",
        "    return timestamp\n",
        "\n",
        "def get_hourly_chime(dt, step=0, rounding_level=\"s\"):\n",
        "    \"\"\"\n",
        "    计算整分钟，整小时，整天的时间\n",
        "    :param step: 往前或往后跳跃取整值，默认为0，即当前所在的时间，正数为往后，负数往前。\n",
        "                例如：\n",
        "                step = 0 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:21\n",
        "                step = 1 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:22\n",
        "                step = -1 时 2019-04-11 17:38:21.869993 取整秒后为 2019-04-11 17:38:20\n",
        "    :param rounding_level: 字符串格式。\n",
        "                \"s\": 按秒取整；\"min\": 按分钟取整；\"hour\": 按小时取整；\"days\": 按天取整\n",
        "    :return: 整理后的时间戳\n",
        "    \"\"\"\n",
        "    if rounding_level == \"days\":  # 整天\n",
        "        td = timedelta(days=-step, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=dt.minute, hours=dt.hour, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"hour\":  # 整小时\n",
        "        td = timedelta(days=0, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=dt.minute, hours=-step, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"min\":  # 整分钟\n",
        "        td = timedelta(days=0, seconds=dt.second, microseconds=dt.microsecond, milliseconds=0, minutes=-step, hours=0, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    elif rounding_level == \"s\":  # 整秒\n",
        "        td = timedelta(days=0, seconds=-step, microseconds=dt.microsecond, milliseconds=0, minutes=0, hours=0, weeks=0)\n",
        "        new_dt = dt - td\n",
        "    else:\n",
        "        new_dt = dt\n",
        "    # timestamp = new_dt.timestamp()  # 对于 python 3 可以直接使用 timestamp 获取时间戳\n",
        "    timestamp = (new_dt - datetime.fromtimestamp(0)).total_seconds()  # Python 2 需手动换算\n",
        "    return timestamp\n",
        "def myappl(data):\n",
        "    if data['时间'] !=None:\n",
        "        t =time.mktime(time.strptime(data['时间'], \"%Y/%m/%d %H:%M\"))\n",
        "        second = get_hourly_chime(dt=datetime.fromtimestamp(t), step=0, rounding_level=\"hour\")\n",
        "    return second\n",
        "def myappl2(data):\n",
        "    if 'CO' in data.index :\n",
        "        return time.strftime(\"%Y/%m/%d %H:%M\", time.localtime(data.name))\n",
        "\n",
        "def load_data(data, seq_len):\n",
        "    print('data len:',len(data))       #4172\n",
        "    print('sequence len:',seq_len)     #50\n",
        " \n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])  #得到长度为seq_len+1的向量，最后一个作为label\n",
        " \n",
        "    print('result len:',len(result))   #4121\n",
        "    print('result shape:',np.array(result).shape)  #（4121,51）\n",
        " \n",
        "    result = np.array(result,dtype= np.float32)\n",
        "    #划分train、test\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    \n",
        "    \n",
        "    train = torch.from_numpy(result[:row])\n",
        "    test = torch.from_numpy(result[row:])\n",
        "    return train, test\n",
        "\n",
        "def pic(y_test,y_hat,name):\n",
        "    mpl.rcParams['font.sans-serif'] = ['simHei']\n",
        "    mpl.rcParams['axes.unicode_minus'] = False\n",
        "    plt.figure(figsize=(10,8),facecolor='w')\n",
        "    plt.plot(y_test, 'r-', linewidth=2, label='origional')\n",
        "    plt.plot(y_hat, 'g-', linewidth=2, label='prediction')\n",
        "    plt.title(name, fontsize=18)\n",
        "#     plt.xticks([])\n",
        "#     plt.yticks([])\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(b=True, ls=':')\n",
        "     \n",
        "def train_prediction(train ,test,learning_rate,name,wh):\n",
        "    hidden_size =16 \n",
        "    seq_length = 1 \n",
        "    batch_size = 51\n",
        "    last =  train[0][-1],train[1][-1]\n",
        "    all = train[0][:,-1],train[1][:,-1]\n",
        "       \n",
        "    model = Lstm(train[0].shape[2] ,hidden_size)\n",
        "    print(model)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "     #out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "#     hidden_prev = torch.zeros(4, batch_size, hidden_size)\n",
        "    for i in range(len(train[0])):\n",
        "        x = train[0][i].view(batch_size, 1 , -1)\n",
        "        y = train[1][i].view(batch_size, seq_length , 1)\n",
        "        output, hidden_prev = model(x)\n",
        "#         hidden_prev = hidden_prev.detach()\n",
        "        \n",
        "        loss = criterion(output, y) # 用前49个点去学习，并用学得的曲线输出即为后49个点并与后49个点真值进行求损失\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        # for p in model.parameters():\n",
        "        #     print(p.grad.norm())\n",
        "        # torch.nn.utils.clip_grad_norm_(p, 10)\n",
        "        optimizer.step()\n",
        "        if i % 200 == 0:\n",
        "            print(\"Iteration: {} \\t loss {}\".format(i, loss.item()))\n",
        "    predictions = []\n",
        "    orig = []\n",
        "    \n",
        "    \n",
        "    \n",
        "    pred, _ = model(all[0].unsqueeze(1)) \n",
        "    predictions.extend(pred.squeeze().detach().numpy())\n",
        "    orig.extend(all[1].numpy())\n",
        "    minput =all[0][-1]\n",
        "    for i in range(24):\n",
        "        minput = minput.view(1, 1, -1)\n",
        "        pred,_ = model(minput)\n",
        "        minput = minput+np.random.rand()\n",
        "        predictions.append(pred.detach().numpy().ravel()[0]) \n",
        "    dd = pd.DataFrame(data=list(zip(orig,predictions)) ,columns=[ '真实','预测'])\n",
        "     \n",
        "    dd.to_csv('{}_{}_{}.csv'.format(name,np.random.randint(1,6,1),wh))\n",
        "    pic(orig,predictions,name)\n",
        "\n",
        "class Lstm(nn.Module):\n",
        "    def __init__(self,INPUT_SIZE,hidden_size):\n",
        "        super(Lstm,self).__init__()\n",
        "        self.lstm = nn.LSTM( \n",
        "            input_size=INPUT_SIZE,\n",
        "            hidden_size=hidden_size,     # rnn hidden unit\n",
        "            num_layers=4,               # number of rnn layer\n",
        "            batch_first=True,           # input & output will has (batch,1s dimension. e.g.\n",
        "            dropout=0.2                 #(batch, time_step, input_size) time_step, input_size)\n",
        "        )\n",
        "        for p in self.lstm.parameters():\n",
        "            nn.init.normal_(p, mean=0.0, std=0.001)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x):    \n",
        "        out, (hidden_prev, c) = self.lstm(x)  #hidden_prev.shape[layer, batch_size, hidden_size]\n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        t = out.size(0)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        out = out.unsqueeze(-1)\n",
        "        return out, (hidden_prev, c)\n",
        "  \n",
        "\n",
        "# Hyper Parameters\n",
        "torch.manual_seed(1)    # reproducible\n",
        "TIME_STEP = 1      # rnn time step\n",
        "INPUT_SIZE = 1      # rnn input size\n",
        "hidden_size = 16\n",
        "LR = 0.02           # learning rate\n",
        "\n",
        " \n",
        "data1 = pd.read_csv('/content/drive/My Drive/tmp/附件1.csv')\n",
        "data2 = pd.read_csv('/content/drive/My Drive/tmp/附件2.csv') \n",
        "data2['hsecond'] = data2.apply(myappl, axis=1)\n",
        "\n",
        "output_size =1\n",
        "learning_rate = 0.01\n",
        "num_epochs =1\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train1,d_test1 = load_data(result, 50 )\n",
        "x_train1, y_train1, x_test1, y_test1  = d_train1[:,:,1:],d_train1[:,:,0],d_test1[:,:,1:],d_test1[:,:,0]\n",
        " \n",
        "train_prediction((x_train1, y_train1) ,(x_test1, y_test1),learning_rate,'PM2.5','国')\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['PM10']\n",
        "ddd.drop(['PM10'],axis=1,inplace=True)\n",
        "ddd['PM10'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'PM10','国')\n",
        "\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['CO']\n",
        "ddd.drop(['CO'],axis=1,inplace=True)\n",
        "ddd['CO'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'CO','国')\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['NO2']\n",
        "ddd.drop(['NO2'],axis=1,inplace=True)\n",
        "ddd['NO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'NO2','国')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['SO2']\n",
        "ddd.drop(['SO2'],axis=1,inplace=True)\n",
        "ddd['SO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'SO2','国')\n",
        "\n",
        "ddd =data1.drop(['时间'],axis=1)\n",
        "tmp = ddd['O3']\n",
        "ddd.drop(['O3'],axis=1,inplace=True)\n",
        "ddd['O3'] = tmp\n",
        "result = MinMaxScaler().fit_transform(ddd.values)\n",
        "d_train12, d_test12  = load_data(result, 50 )\n",
        "x_train12, y_train12, x_test12, y_test12  = d_train12[:,:,0:-1],d_train12[:,:,-1],d_test12[:,:,0:-1],d_test12[:,:,-1]\n",
        "train_prediction((x_train12, y_train12) ,(x_test12, y_test12),learning_rate,'O3','国')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data2 = data2.groupby('hsecond').mean()\n",
        "# data2['时间']  = data2.apply(myappl2,axis=1)\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train2 , d_test2 = load_data(result, 50)\n",
        "x_train2, y_train2, x_test2, y_test2  = d_train2[:,:,1:11],d_train2[:,:,0],d_test2[:,:,1:11],d_test2[:,:,0]\n",
        "\n",
        "train_prediction((x_train2, y_train2) ,(x_test2, y_test2 ),learning_rate,'PM2.5','自')\n",
        "\n",
        "\n",
        "tmp = data2['PM10']\n",
        "data2.drop(['PM10'],axis=1,inplace=True)\n",
        "data2['PM10'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'PM10','自')   \n",
        "        \n",
        "\n",
        "tmp = data2['CO']\n",
        "data2.drop(['CO'],axis=1,inplace=True)\n",
        "data2['CO'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'CO','自')     \n",
        "  \n",
        "\n",
        "\n",
        "tmp = data2['NO2']\n",
        "data2.drop(['NO2'],axis=1,inplace=True)\n",
        "data2['NO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'NO2','自')   \n",
        "        \n",
        "\n",
        "tmp = data2['SO2']\n",
        "data2.drop(['SO2'],axis=1,inplace=True)\n",
        "data2['SO2'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'SO2','自')      \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "tmp = data2['O3']\n",
        "data2.drop(['O3'],axis=1,inplace=True)\n",
        "data2['O3'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'O3','自')   \n",
        "        \n",
        "\n",
        "tmp = data2['风速']\n",
        "data2.drop(['风速'],axis=1,inplace=True)\n",
        "data2['风速'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'windSpeed','自')    \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "tmp = data2['压强']\n",
        "data2.drop(['压强'],axis=1,inplace=True)\n",
        "data2['压强'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'pressure','自')   \n",
        "        \n",
        "\n",
        "tmp = data2['降水量']\n",
        "data2.drop(['降水量'],axis=1,inplace=True)\n",
        "data2['降水量'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'precipitation','自')    \n",
        "  \n",
        "  \n",
        "tmp = data2['温度']\n",
        "data2.drop(['温度'],axis=1,inplace=True)\n",
        "data2['温度'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'temperature','自')   \n",
        "        \n",
        "\n",
        "tmp = data2['湿度']\n",
        "data2.drop(['湿度'],axis=1,inplace=True)\n",
        "data2['湿度'] = tmp\n",
        "result = MinMaxScaler().fit_transform(data2.values)\n",
        "d_train22, d_test22  = load_data(result, 50 )\n",
        "x_train22, y_train22, x_test22, y_test22 = d_train22[:,:,0:-1],d_train22[:,:,-1],d_test22[:,:,0:-1],d_test22[:,:,-1]\n",
        "train_prediction((x_train22, y_train22) ,(x_test22, y_test22),learning_rate,'humidity','自') \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#PM2.5,PM10,CO,NO2,SO2,O3,风速,压强,降水量,温度,湿度\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "# x = torch.randn(10, 3, 100)\n",
        "# out, (h, c) = lstm(x)\n",
        "# print(out.shape, h.shape, c.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.004163509234786034\n",
            "Iteration: 200 \t loss 0.0022360114380717278\n",
            "Iteration: 400 \t loss 0.004413819871842861\n",
            "Iteration: 600 \t loss 0.002702730242162943\n",
            "Iteration: 800 \t loss 0.0011058201780542731\n",
            "Iteration: 1000 \t loss 0.0003594347508624196\n",
            "Iteration: 1200 \t loss 0.0011999821290373802\n",
            "Iteration: 1400 \t loss 0.003111378289759159\n",
            "Iteration: 1600 \t loss 0.0016811952227726579\n",
            "Iteration: 1800 \t loss 0.0026465619448572397\n",
            "Iteration: 2000 \t loss 0.0003380585403647274\n",
            "Iteration: 2200 \t loss 0.002430796390399337\n",
            "Iteration: 2400 \t loss 0.0030681700445711613\n",
            "Iteration: 2600 \t loss 0.0012634714366868138\n",
            "Iteration: 2800 \t loss 0.0017792786238715053\n",
            "Iteration: 3000 \t loss 0.0007517727208323777\n",
            "Iteration: 3200 \t loss 0.0018285379046574235\n",
            "Iteration: 3400 \t loss 0.0009693647152744234\n",
            "Iteration: 3600 \t loss 0.0006049996591173112\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.005552304908633232\n",
            "Iteration: 200 \t loss 0.00039171887328848243\n",
            "Iteration: 400 \t loss 0.000287643721094355\n",
            "Iteration: 600 \t loss 0.00013159493391867727\n",
            "Iteration: 800 \t loss 0.00019252835772931576\n",
            "Iteration: 1000 \t loss 0.00012545128993224353\n",
            "Iteration: 1200 \t loss 9.872904774965718e-05\n",
            "Iteration: 1400 \t loss 0.00019426480866968632\n",
            "Iteration: 1600 \t loss 0.00010315011604689062\n",
            "Iteration: 1800 \t loss 0.0002926344168372452\n",
            "Iteration: 2000 \t loss 6.601947825402021e-05\n",
            "Iteration: 2200 \t loss 0.0003308110754005611\n",
            "Iteration: 2400 \t loss 0.00027792443870566785\n",
            "Iteration: 2600 \t loss 0.0001213279101648368\n",
            "Iteration: 2800 \t loss 0.0001777450233930722\n",
            "Iteration: 3000 \t loss 7.809011003701016e-05\n",
            "Iteration: 3200 \t loss 0.015385978855192661\n",
            "Iteration: 3400 \t loss 0.0003371565544512123\n",
            "Iteration: 3600 \t loss 0.000257766165304929\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.015298710204660892\n",
            "Iteration: 200 \t loss 0.0024048862978816032\n",
            "Iteration: 400 \t loss 0.004251732956618071\n",
            "Iteration: 600 \t loss 0.0023231019731611013\n",
            "Iteration: 800 \t loss 0.0022901520133018494\n",
            "Iteration: 1000 \t loss 0.0014786553801968694\n",
            "Iteration: 1200 \t loss 0.0020308280363678932\n",
            "Iteration: 1400 \t loss 0.0025947117246687412\n",
            "Iteration: 1600 \t loss 0.0011744166258722544\n",
            "Iteration: 1800 \t loss 0.002624065848067403\n",
            "Iteration: 2000 \t loss 0.0012193989241495728\n",
            "Iteration: 2200 \t loss 0.003020991338416934\n",
            "Iteration: 2400 \t loss 0.009090843610465527\n",
            "Iteration: 2600 \t loss 0.0043446701020002365\n",
            "Iteration: 2800 \t loss 0.0031025325879454613\n",
            "Iteration: 3000 \t loss 0.001257537747733295\n",
            "Iteration: 3200 \t loss 0.0016198176890611649\n",
            "Iteration: 3400 \t loss 0.001405927469022572\n",
            "Iteration: 3600 \t loss 0.0005104744923301041\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.004975159652531147\n",
            "Iteration: 200 \t loss 0.0005398556822910905\n",
            "Iteration: 400 \t loss 4.843859278480522e-05\n",
            "Iteration: 600 \t loss 0.0002393425820628181\n",
            "Iteration: 800 \t loss 0.00010346915223635733\n",
            "Iteration: 1000 \t loss 0.0004894891171716154\n",
            "Iteration: 1200 \t loss 0.004246922675520182\n",
            "Iteration: 1400 \t loss 0.008207590319216251\n",
            "Iteration: 1600 \t loss 0.01043765340000391\n",
            "Iteration: 1800 \t loss 0.005354444030672312\n",
            "Iteration: 2000 \t loss 0.002663718070834875\n",
            "Iteration: 2200 \t loss 0.009279574267566204\n",
            "Iteration: 2400 \t loss 0.012942426837980747\n",
            "Iteration: 2600 \t loss 0.017434369772672653\n",
            "Iteration: 2800 \t loss 0.016124192625284195\n",
            "Iteration: 3000 \t loss 0.007715069688856602\n",
            "Iteration: 3200 \t loss 0.00804079044610262\n",
            "Iteration: 3400 \t loss 0.00467778230085969\n",
            "Iteration: 3600 \t loss 0.007447561714798212\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.01498192548751831\n",
            "Iteration: 200 \t loss 0.014256632886826992\n",
            "Iteration: 400 \t loss 0.004748198203742504\n",
            "Iteration: 600 \t loss 0.007738957181572914\n",
            "Iteration: 800 \t loss 0.003029612824320793\n",
            "Iteration: 1000 \t loss 0.004602394066751003\n",
            "Iteration: 1200 \t loss 0.0012897650012746453\n",
            "Iteration: 1400 \t loss 0.00018040169379673898\n",
            "Iteration: 1600 \t loss 0.0002517569519113749\n",
            "Iteration: 1800 \t loss 0.0001489127753302455\n",
            "Iteration: 2000 \t loss 0.00017321744235232472\n",
            "Iteration: 2200 \t loss 0.0003746233705896884\n",
            "Iteration: 2400 \t loss 4.1143201087834314e-05\n",
            "Iteration: 2600 \t loss 0.0007650107727386057\n",
            "Iteration: 2800 \t loss 0.0009992563864216208\n",
            "Iteration: 3000 \t loss 0.00021039263810962439\n",
            "Iteration: 3200 \t loss 0.0002499820839148015\n",
            "Iteration: 3400 \t loss 0.0005392185994423926\n",
            "Iteration: 3600 \t loss 0.00033417780650779605\n",
            "data len: 4200\n",
            "sequence len: 50\n",
            "result len: 4149\n",
            "result shape: (4149, 51, 6)\n",
            "Lstm(\n",
            "  (lstm): LSTM(5, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.0069947801530361176\n",
            "Iteration: 200 \t loss 0.00580115569755435\n",
            "Iteration: 400 \t loss 0.000621150596998632\n",
            "Iteration: 600 \t loss 0.0029732452239841223\n",
            "Iteration: 800 \t loss 0.0014962280401960015\n",
            "Iteration: 1000 \t loss 0.0012164557119831443\n",
            "Iteration: 1200 \t loss 0.0024590077809989452\n",
            "Iteration: 1400 \t loss 0.005077512003481388\n",
            "Iteration: 1600 \t loss 0.0019983076490461826\n",
            "Iteration: 1800 \t loss 0.006609502248466015\n",
            "Iteration: 2000 \t loss 0.0008043267298489809\n",
            "Iteration: 2200 \t loss 0.007269037887454033\n",
            "Iteration: 2400 \t loss 0.002561806235462427\n",
            "Iteration: 2600 \t loss 0.004090214613825083\n",
            "Iteration: 2800 \t loss 0.006038290448486805\n",
            "Iteration: 3000 \t loss 0.006971579510718584\n",
            "Iteration: 3200 \t loss 0.009642375633120537\n",
            "Iteration: 3400 \t loss 0.011844287626445293\n",
            "Iteration: 3600 \t loss 0.015446143224835396\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.002717098454013467\n",
            "Iteration: 200 \t loss 0.0010708656627684832\n",
            "Iteration: 400 \t loss 0.0010653870413079858\n",
            "Iteration: 600 \t loss 0.0021210925187915564\n",
            "Iteration: 800 \t loss 0.0006656951736658812\n",
            "Iteration: 1000 \t loss 0.0003420866560190916\n",
            "Iteration: 1200 \t loss 0.0011531212367117405\n",
            "Iteration: 1400 \t loss 0.00021481576550286263\n",
            "Iteration: 1600 \t loss 0.002861534245312214\n",
            "Iteration: 1800 \t loss 0.000331397372065112\n",
            "Iteration: 2000 \t loss 0.00036165141500532627\n",
            "Iteration: 2200 \t loss 0.0006357391248457134\n",
            "Iteration: 2400 \t loss 0.00044308186625130475\n",
            "Iteration: 2600 \t loss 0.00027720851358026266\n",
            "Iteration: 2800 \t loss 0.0003378801338840276\n",
            "Iteration: 3000 \t loss 0.00038954810588620603\n",
            "Iteration: 3200 \t loss 0.000372228620108217\n",
            "Iteration: 3400 \t loss 0.0002806098200380802\n",
            "Iteration: 3600 \t loss 0.00026334673748351634\n",
            "Iteration: 3800 \t loss 0.0001439016341464594\n",
            "Iteration: 4000 \t loss 0.0002265429648105055\n",
            "Iteration: 4200 \t loss 0.0003070971288252622\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.0900115966796875\n",
            "Iteration: 200 \t loss 0.0005616391426883638\n",
            "Iteration: 400 \t loss 0.002602170454338193\n",
            "Iteration: 600 \t loss 0.004019798710942268\n",
            "Iteration: 800 \t loss 0.000556581187993288\n",
            "Iteration: 1000 \t loss 0.00024455678067170084\n",
            "Iteration: 1200 \t loss 0.0009975827997550368\n",
            "Iteration: 1400 \t loss 0.00015657345647923648\n",
            "Iteration: 1600 \t loss 0.0026063453406095505\n",
            "Iteration: 1800 \t loss 0.0003282993275206536\n",
            "Iteration: 2000 \t loss 0.00022945422097109258\n",
            "Iteration: 2200 \t loss 0.00044704016181640327\n",
            "Iteration: 2400 \t loss 0.00012516741116996855\n",
            "Iteration: 2600 \t loss 0.00021929088688921183\n",
            "Iteration: 2800 \t loss 0.0002550379140302539\n",
            "Iteration: 3000 \t loss 0.0001739324361551553\n",
            "Iteration: 3200 \t loss 0.00011855649063363671\n",
            "Iteration: 3400 \t loss 0.00011535953672137111\n",
            "Iteration: 3600 \t loss 7.548792200395837e-05\n",
            "Iteration: 3800 \t loss 7.552729948656633e-05\n",
            "Iteration: 4000 \t loss 3.655520413303748e-05\n",
            "Iteration: 4200 \t loss 0.00013188563752919436\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.08584168553352356\n",
            "Iteration: 200 \t loss 0.0023954990319907665\n",
            "Iteration: 400 \t loss 0.0004926169058308005\n",
            "Iteration: 600 \t loss 0.0002155363908968866\n",
            "Iteration: 800 \t loss 0.0013293330557644367\n",
            "Iteration: 1000 \t loss 8.864569826982915e-05\n",
            "Iteration: 1200 \t loss 0.000850162876304239\n",
            "Iteration: 1400 \t loss 0.002171610714867711\n",
            "Iteration: 1600 \t loss 0.00959409773349762\n",
            "Iteration: 1800 \t loss 0.0016072659054771066\n",
            "Iteration: 2000 \t loss 4.714227543445304e-05\n",
            "Iteration: 2200 \t loss 9.750840399647132e-05\n",
            "Iteration: 2400 \t loss 0.00012554271961562335\n",
            "Iteration: 2600 \t loss 0.0001070949001586996\n",
            "Iteration: 2800 \t loss 0.006042988505214453\n",
            "Iteration: 3000 \t loss 0.00019522862567100674\n",
            "Iteration: 3200 \t loss 0.00037384385359473526\n",
            "Iteration: 3400 \t loss 0.00023840233916416764\n",
            "Iteration: 3600 \t loss 0.0008197968709282577\n",
            "Iteration: 3800 \t loss 0.00016928747936617583\n",
            "Iteration: 4000 \t loss 0.00039046531310305\n",
            "Iteration: 4200 \t loss 0.00040877796709537506\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.036662161350250244\n",
            "Iteration: 200 \t loss 0.006649554241448641\n",
            "Iteration: 400 \t loss 0.001830012653954327\n",
            "Iteration: 600 \t loss 0.003290431573987007\n",
            "Iteration: 800 \t loss 0.002337839687243104\n",
            "Iteration: 1000 \t loss 0.001302347402088344\n",
            "Iteration: 1200 \t loss 0.001605786383152008\n",
            "Iteration: 1400 \t loss 0.0033083378802984953\n",
            "Iteration: 1600 \t loss 0.011478058993816376\n",
            "Iteration: 1800 \t loss 0.0015859613195061684\n",
            "Iteration: 2000 \t loss 1.5199237168417312e-05\n",
            "Iteration: 2200 \t loss 0.011088581755757332\n",
            "Iteration: 2400 \t loss 0.018089788034558296\n",
            "Iteration: 2600 \t loss 0.009365750476717949\n",
            "Iteration: 2800 \t loss 0.02059089206159115\n",
            "Iteration: 3000 \t loss 0.008792783133685589\n",
            "Iteration: 3200 \t loss 0.01387789100408554\n",
            "Iteration: 3400 \t loss 0.004451580345630646\n",
            "Iteration: 3600 \t loss 0.005457697436213493\n",
            "Iteration: 3800 \t loss 0.00328855705447495\n",
            "Iteration: 4000 \t loss 0.001714628771878779\n",
            "Iteration: 4200 \t loss 0.018534639850258827\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.0453948900103569\n",
            "Iteration: 200 \t loss 4.805551725439727e-06\n",
            "Iteration: 400 \t loss 1.5458324469364015e-06\n",
            "Iteration: 600 \t loss 3.892342874678434e-07\n",
            "Iteration: 800 \t loss 3.110021862084977e-05\n",
            "Iteration: 1000 \t loss 2.691460565529269e-07\n",
            "Iteration: 1200 \t loss 6.58269982523052e-07\n",
            "Iteration: 1400 \t loss 2.8768508855137043e-05\n",
            "Iteration: 1600 \t loss 9.023361053550616e-05\n",
            "Iteration: 1800 \t loss 8.925210750021506e-06\n",
            "Iteration: 2000 \t loss 4.2384699554531835e-06\n",
            "Iteration: 2200 \t loss 3.693790176839684e-06\n",
            "Iteration: 2400 \t loss 1.9460703697404824e-06\n",
            "Iteration: 2600 \t loss 5.185409008845454e-06\n",
            "Iteration: 2800 \t loss 1.1741794878616929e-05\n",
            "Iteration: 3000 \t loss 3.602840024541365e-06\n",
            "Iteration: 3200 \t loss 4.931936473440146e-06\n",
            "Iteration: 3400 \t loss 1.505320028627466e-06\n",
            "Iteration: 3600 \t loss 4.4266239456192125e-06\n",
            "Iteration: 3800 \t loss 1.3911705991631607e-06\n",
            "Iteration: 4000 \t loss 1.7034873280863394e-06\n",
            "Iteration: 4200 \t loss 3.989995548181469e-06\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.15789225697517395\n",
            "Iteration: 200 \t loss 0.009064842015504837\n",
            "Iteration: 400 \t loss 0.0009541327017359436\n",
            "Iteration: 600 \t loss 0.0009000561549328268\n",
            "Iteration: 800 \t loss 0.002213540719822049\n",
            "Iteration: 1000 \t loss 0.004169011488556862\n",
            "Iteration: 1200 \t loss 0.00021290543372742832\n",
            "Iteration: 1400 \t loss 0.0022932940628379583\n",
            "Iteration: 1600 \t loss 0.001003357581794262\n",
            "Iteration: 1800 \t loss 0.0005833584000356495\n",
            "Iteration: 2000 \t loss 0.00043232180178165436\n",
            "Iteration: 2200 \t loss 0.0005528416368179023\n",
            "Iteration: 2400 \t loss 0.001924049574881792\n",
            "Iteration: 2600 \t loss 0.0015740342205390334\n",
            "Iteration: 2800 \t loss 0.002692734356969595\n",
            "Iteration: 3000 \t loss 0.001674454309977591\n",
            "Iteration: 3200 \t loss 0.0026076561771333218\n",
            "Iteration: 3400 \t loss 0.005528867710381746\n",
            "Iteration: 3600 \t loss 0.005429130978882313\n",
            "Iteration: 3800 \t loss 0.004775102250277996\n",
            "Iteration: 4000 \t loss 0.006824762560427189\n",
            "Iteration: 4200 \t loss 0.015627196058630943\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.030284063890576363\n",
            "Iteration: 200 \t loss 0.038134075701236725\n",
            "Iteration: 400 \t loss 0.005420393776148558\n",
            "Iteration: 600 \t loss 0.007334959227591753\n",
            "Iteration: 800 \t loss 0.0090732267126441\n",
            "Iteration: 1000 \t loss 0.0147958779707551\n",
            "Iteration: 1200 \t loss 0.003258724231272936\n",
            "Iteration: 1400 \t loss 0.02136065997183323\n",
            "Iteration: 1600 \t loss 0.04524289816617966\n",
            "Iteration: 1800 \t loss 0.015195352025330067\n",
            "Iteration: 2000 \t loss 0.00568169541656971\n",
            "Iteration: 2200 \t loss 0.008490579202771187\n",
            "Iteration: 2400 \t loss 0.007912985980510712\n",
            "Iteration: 2600 \t loss 0.021825427189469337\n",
            "Iteration: 2800 \t loss 0.0233653262257576\n",
            "Iteration: 3000 \t loss 0.01795901358127594\n",
            "Iteration: 3200 \t loss 0.02147573232650757\n",
            "Iteration: 3400 \t loss 0.011320949532091618\n",
            "Iteration: 3600 \t loss 0.028048768639564514\n",
            "Iteration: 3800 \t loss 0.0155458003282547\n",
            "Iteration: 4000 \t loss 0.026012476533651352\n",
            "Iteration: 4200 \t loss 0.006357963662594557\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.4357096254825592\n",
            "Iteration: 200 \t loss 0.0026612207293510437\n",
            "Iteration: 400 \t loss 0.003922921605408192\n",
            "Iteration: 600 \t loss 0.003504459047690034\n",
            "Iteration: 800 \t loss 0.00039300386561080813\n",
            "Iteration: 1000 \t loss 0.010663236491382122\n",
            "Iteration: 1200 \t loss 0.0009598602191545069\n",
            "Iteration: 1400 \t loss 0.0019206394208595157\n",
            "Iteration: 1600 \t loss 0.011600679717957973\n",
            "Iteration: 1800 \t loss 0.013209471479058266\n",
            "Iteration: 2000 \t loss 0.0014525180449709296\n",
            "Iteration: 2200 \t loss 0.001699640997685492\n",
            "Iteration: 2400 \t loss 0.0004368474765215069\n",
            "Iteration: 2600 \t loss 0.0047107478603720665\n",
            "Iteration: 2800 \t loss 0.0010718273697420955\n",
            "Iteration: 3000 \t loss 0.0022010316606611013\n",
            "Iteration: 3200 \t loss 0.002392757451161742\n",
            "Iteration: 3400 \t loss 0.00197958922944963\n",
            "Iteration: 3600 \t loss 0.0020771524868905544\n",
            "Iteration: 3800 \t loss 0.0033240485936403275\n",
            "Iteration: 4000 \t loss 0.0015985023928806186\n",
            "Iteration: 4200 \t loss 0.0005948477191850543\n",
            "data len: 4896\n",
            "sequence len: 50\n",
            "result len: 4845\n",
            "result shape: (4845, 51, 11)\n",
            "Lstm(\n",
            "  (lstm): LSTM(10, 16, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "Iteration: 0 \t loss 0.20615336298942566\n",
            "Iteration: 200 \t loss 1.1062797057093121e-05\n",
            "Iteration: 400 \t loss 6.743385165464133e-05\n",
            "Iteration: 600 \t loss 0.00018877470574807376\n",
            "Iteration: 800 \t loss 0.0021932143718004227\n",
            "Iteration: 1000 \t loss 6.543158815475181e-05\n",
            "Iteration: 1200 \t loss 2.7473630325403064e-05\n",
            "Iteration: 1400 \t loss 0.000930583628360182\n",
            "Iteration: 1600 \t loss 4.4684834392683115e-06\n",
            "Iteration: 1800 \t loss 6.742393907188671e-06\n",
            "Iteration: 2000 \t loss 7.645093319297303e-06\n",
            "Iteration: 2200 \t loss 0.00016213326307479292\n",
            "Iteration: 2400 \t loss 5.954988591838628e-05\n",
            "Iteration: 2600 \t loss 5.609948857454583e-06\n",
            "Iteration: 2800 \t loss 1.6354109675376094e-06\n",
            "Iteration: 3000 \t loss 3.5188202218705555e-07\n",
            "Iteration: 3200 \t loss 7.101427286215767e-07\n",
            "Iteration: 3400 \t loss 1.603834425623063e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHzP06oVDNJb",
        "colab_type": "code",
        "outputId": "62161622-54fc-4b2a-e48b-77a940f00e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}